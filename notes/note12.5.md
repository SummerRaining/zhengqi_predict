https://tianchi.aliyun.com/competition/entrance/231693/introduction
今日计划：
0. 树形模型的默认参数，max\_depth,learning\_rate,n\_estimators,subsample。和调整好的参数，上传到github。<完成！>
1. 偏态纠正操作，查看去掉该操作后是否减弱模型，如果有用了解原理。
	1. 实验4,不使用纠正偏态分布操作。
		- krr2 得分: 0.1337 (0.0233)，线上得分:0.1294
		- svr 得分: 0.1347 (0.0254), 线上得分:0.1327
		- nn 得分: 0.1572 (0.0216), 线上得分:0.1527
		- xgboost 得分: 0.1333 (0.0292), 线上得分:0.1431
		- lgbm 得分: 0.1363 (0.0281), 线上得分:0.1397
	2. 结果整理：krr2,svr,nn的线上线下结果都减少了0.01～0.03，线上结果减弱的程度比线下高。lgbm,xgboost线下结果没有改变，但是线上结果变差了分别减少0.004和0.011。
	3. 结果分析：偏态分布纠正对线性模型，会增强其拟合能力和线上泛化能力。对树形集成模型，可以增强其线上泛化能力。且对线性模型的提升大于对树形模型的提升。
	4. 总结：纠正偏态分布的操作，非常重要。几乎对__所有模型都有效__。需要学习判断特征偏态的方法和纠正方法。
2. 标准化操作，与偏态操作同时进行。
	- 由于偏态分布纠正依赖标准化，取exp和log操作需要对标准化的数据做，不然数值会非常大。
	- 实验4,同时去掉标准化操作和偏态分布纠正操作。
		- krr2 得分: 0.1337 (0.0233)，线上得分:0.1294
		- svr 得分: 0.1348 (0.0253),线上得分:0.1327
		- nn 得分: 0.1461 (0.0337)，线上得分:0.1579
		- xgboost 得分: 0.1328 (0.0290)，线上得分:0.1427
		- lgbm 得分: 0.1355 (0.0283)，线上得分:0.1397
	- 结果整理：与不进行偏态纠正但是标准化的实验对比，__krr2,svr没有任何变化__，可能算法本身拟合时就做了标准化，nn的线上得分有所下降。lgbm和xgboost得分几乎没有变化，它们的训练与特征的范围无关。
	- 结果分析：单独标准化没有效果，标准化应该作为偏态纠正的前置操作。标准化和偏态纠正同时进行能大大提高线性模型的拟合能力和泛化能力，也能显著提高树形模型的泛化能力。
	- 总结：对连续的特征，建议都__先做标准化然后纠正偏态分布__。
3. 总结实验操作:
	1. 删除训练集和线上测试集分布不一样的特征，这一步对树形模型无效，对线性模型很重要。原因：树形模型可以自己选择重要的特征，而不被无用特征影响。
	2. 纠正偏态分布和标准化，对线性模型和树形模型都有提升。原因：纠正偏态分布起了作用，标准化是是偏态分布纠正的前置操作，高斯分布让特征取值更均匀对称，让模型利用特征的值。
	3. 单变量筛选和方差筛选配合使用对__线性模型(SVR,KRR2,NN)非常重要！__，主要表现在增强模型的泛化能力，减小线上线下的差异。注意：这一步对线下没有提升。原因:线性模型不能有效的过滤无用特征，其简单的结果天生具有很好的泛化能力。
	4. 单变量筛选和方差的使用对__树形结构的模型(LGBM,Xgboost)无效反而有损害__。原因：树形模型可以自己选择重要的特征，而不被无用特征影响，且复杂的集成模型可以利用到相关性较低特征中的一些细小信息。
4. 改进方法：增加二次交叉项特征，google添加方法，对所有特征同样做标准化，纠正偏态分布，方差选择和单变量选择，尝试最优特征数，线下原则是特征越少越好。线性模型泛化能力好，但是结构太简单不能抓住重要信息。
5. 树形模型调参，然后融合提交。
6. 尝试删除训练时的异常点是否有用。

6. 开始准备毕业论文，阅读上次报告后的doc文件，总结需要做的工作。
7. 网上看一件外套，让老妈给钱。要冻死了。
